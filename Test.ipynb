{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Iteration 0, loss = 4.8799\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 6 / 4600 correct (0.13)\n",
      "\n",
      "Iteration 25, loss = 4.2439\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 172 / 4600 correct (3.74)\n",
      "\n",
      "Iteration 50, loss = 4.0894\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 169 / 4600 correct (3.67)\n",
      "\n",
      "Iteration 125, loss = 4.0561\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 195 / 4600 correct (4.24)\n",
      "\n",
      "Iteration 150, loss = 4.0677\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 183 / 4600 correct (3.98)\n",
      "\n",
      "Iteration 175, loss = 3.9488\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 169 / 4600 correct (3.67)\n",
      "\n",
      "Iteration 200, loss = 4.0362\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 170 / 4600 correct (3.70)\n",
      "\n",
      "Iteration 225, loss = 3.9733\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 210 / 4600 correct (4.57)\n",
      "\n",
      "Iteration 250, loss = 3.9530\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 185 / 4600 correct (4.02)\n",
      "\n",
      "Iteration 275, loss = 3.9283\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 182 / 4600 correct (3.96)\n",
      "\n",
      "Iteration 300, loss = 3.9495\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 184 / 4600 correct (4.00)\n",
      "\n",
      "Iteration 325, loss = 4.0083\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 180 / 4600 correct (3.91)\n",
      "\n",
      "Iteration 350, loss = 4.0708\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 164 / 4600 correct (3.57)\n",
      "\n",
      "Iteration 0, loss = 3.9979\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 169 / 4600 correct (3.67)\n",
      "\n",
      "Iteration 25, loss = 3.9351\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 187 / 4600 correct (4.07)\n",
      "\n",
      "Iteration 50, loss = 3.9439\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 148 / 4600 correct (3.22)\n",
      "\n",
      "Iteration 75, loss = 3.9164\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 154 / 4600 correct (3.35)\n",
      "\n",
      "Iteration 100, loss = 3.9916\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 162 / 4600 correct (3.52)\n",
      "\n",
      "Iteration 125, loss = 3.9356\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 186 / 4600 correct (4.04)\n",
      "\n",
      "Iteration 150, loss = 3.9900\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 160 / 4600 correct (3.48)\n",
      "\n",
      "Iteration 175, loss = 4.0050\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 173 / 4600 correct (3.76)\n",
      "\n",
      "Iteration 200, loss = 4.0037\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 163 / 4600 correct (3.54)\n",
      "\n",
      "Iteration 225, loss = 3.9113\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 169 / 4600 correct (3.67)\n",
      "\n",
      "Iteration 250, loss = 3.8970\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 170 / 4600 correct (3.70)\n",
      "\n",
      "Iteration 275, loss = 3.9558\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 167 / 4600 correct (3.63)\n",
      "\n",
      "Iteration 300, loss = 3.9332\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 160 / 4600 correct (3.48)\n",
      "\n",
      "Iteration 325, loss = 3.9496\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 169 / 4600 correct (3.67)\n",
      "\n",
      "Iteration 350, loss = 3.9958\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 178 / 4600 correct (3.87)\n",
      "\n",
      "Iteration 0, loss = 3.9286\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 162 / 4600 correct (3.52)\n",
      "\n",
      "Iteration 25, loss = 3.9555\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 147 / 4600 correct (3.20)\n",
      "\n",
      "Iteration 50, loss = 3.9680\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 174 / 4600 correct (3.78)\n",
      "\n",
      "Iteration 75, loss = 3.9902\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 100, loss = 3.9711\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 125, loss = 3.8864\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 154 / 4600 correct (3.35)\n",
      "\n",
      "Iteration 150, loss = 3.9207\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 182 / 4600 correct (3.96)\n",
      "\n",
      "Iteration 175, loss = 3.8708\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 157 / 4600 correct (3.41)\n",
      "\n",
      "Iteration 200, loss = 3.9016\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 187 / 4600 correct (4.07)\n",
      "\n",
      "Iteration 225, loss = 3.9077\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 176 / 4600 correct (3.83)\n",
      "\n",
      "Iteration 250, loss = 3.9516\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 178 / 4600 correct (3.87)\n",
      "\n",
      "Iteration 275, loss = 3.9497\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 175 / 4600 correct (3.80)\n",
      "\n",
      "Iteration 300, loss = 3.9011\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 170 / 4600 correct (3.70)\n",
      "\n",
      "Iteration 325, loss = 3.9303\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 188 / 4600 correct (4.09)\n",
      "\n",
      "Iteration 350, loss = 3.9536\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 184 / 4600 correct (4.00)\n",
      "\n",
      "Iteration 0, loss = 3.8568\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 194 / 4600 correct (4.22)\n",
      "\n",
      "Iteration 25, loss = 3.9192\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 170 / 4600 correct (3.70)\n",
      "\n",
      "Iteration 50, loss = 3.8972\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 185 / 4600 correct (4.02)\n",
      "\n",
      "Iteration 75, loss = 3.9184\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 176 / 4600 correct (3.83)\n",
      "\n",
      "Iteration 100, loss = 3.9139\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 177 / 4600 correct (3.85)\n",
      "\n",
      "Iteration 125, loss = 3.8799\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 160 / 4600 correct (3.48)\n",
      "\n",
      "Iteration 150, loss = 3.8749\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 172 / 4600 correct (3.74)\n",
      "\n",
      "Iteration 175, loss = 3.8648\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 177 / 4600 correct (3.85)\n",
      "\n",
      "Iteration 200, loss = 3.9196\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 172 / 4600 correct (3.74)\n",
      "\n",
      "Iteration 225, loss = 3.8609\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 167 / 4600 correct (3.63)\n",
      "\n",
      "Iteration 250, loss = 3.9057\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 183 / 4600 correct (3.98)\n",
      "\n",
      "Iteration 275, loss = 3.8952\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 300, loss = 3.8669\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 186 / 4600 correct (4.04)\n",
      "\n",
      "Iteration 325, loss = 3.8855\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 175 / 4600 correct (3.80)\n",
      "\n",
      "Iteration 350, loss = 3.9206\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 178 / 4600 correct (3.87)\n",
      "\n",
      "Iteration 0, loss = 3.8909\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 175 / 4600 correct (3.80)\n",
      "\n",
      "Iteration 25, loss = 3.8665\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 173 / 4600 correct (3.76)\n",
      "\n",
      "Iteration 50, loss = 3.7986\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 190 / 4600 correct (4.13)\n",
      "\n",
      "Iteration 75, loss = 3.8854\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 187 / 4600 correct (4.07)\n",
      "\n",
      "Iteration 100, loss = 3.8707\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 175 / 4600 correct (3.80)\n",
      "\n",
      "Iteration 125, loss = 3.8152\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 167 / 4600 correct (3.63)\n",
      "\n",
      "Iteration 150, loss = 3.9025\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 178 / 4600 correct (3.87)\n",
      "\n",
      "Iteration 175, loss = 3.8816\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 160 / 4600 correct (3.48)\n",
      "\n",
      "Iteration 200, loss = 3.9076\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 189 / 4600 correct (4.11)\n",
      "\n",
      "Iteration 225, loss = 3.8916\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 168 / 4600 correct (3.65)\n",
      "\n",
      "Iteration 250, loss = 3.8084\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 187 / 4600 correct (4.07)\n",
      "\n",
      "Iteration 275, loss = 3.8712\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 180 / 4600 correct (3.91)\n",
      "\n",
      "Iteration 300, loss = 3.9382\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 169 / 4600 correct (3.67)\n",
      "\n",
      "Iteration 325, loss = 3.8475\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 172 / 4600 correct (3.74)\n",
      "\n",
      "Iteration 350, loss = 3.8977\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 180 / 4600 correct (3.91)\n",
      "\n",
      "Iteration 0, loss = 3.8400\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 211 / 4600 correct (4.59)\n",
      "\n",
      "Iteration 25, loss = 3.9116\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n",
      "Got 165 / 4600 correct (3.59)\n",
      "\n",
      "Iteration 50, loss = 3.9690\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 190 / 4600 correct (4.13)\n",
      "\n",
      "Iteration 75, loss = 3.8478\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 198 / 4600 correct (4.30)\n",
      "\n",
      "Iteration 100, loss = 3.8955\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 178 / 4600 correct (3.87)\n",
      "\n",
      "Iteration 125, loss = 3.9515\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 150, loss = 3.8866\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 175, loss = 3.8950\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 167 / 4600 correct (3.63)\n",
      "\n",
      "Iteration 200, loss = 3.8663\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 180 / 4600 correct (3.91)\n",
      "\n",
      "Iteration 225, loss = 3.8873\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 171 / 4600 correct (3.72)\n",
      "\n",
      "Iteration 250, loss = 3.8319\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 194 / 4600 correct (4.22)\n",
      "\n",
      "Iteration 275, loss = 3.8963\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 197 / 4600 correct (4.28)\n",
      "\n",
      "Iteration 300, loss = 3.8633\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 189 / 4600 correct (4.11)\n",
      "\n",
      "Iteration 325, loss = 3.9247\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 177 / 4600 correct (3.85)\n",
      "\n",
      "Iteration 350, loss = 3.9080\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 174 / 4600 correct (3.78)\n",
      "\n",
      "Iteration 0, loss = 3.8639\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 169 / 4600 correct (3.67)\n",
      "\n",
      "Iteration 25, loss = 3.9111\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 173 / 4600 correct (3.76)\n",
      "\n",
      "Iteration 50, loss = 3.8258\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 169 / 4600 correct (3.67)\n",
      "\n",
      "Iteration 75, loss = 3.8480\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 177 / 4600 correct (3.85)\n",
      "\n",
      "Iteration 100, loss = 3.8292\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 168 / 4600 correct (3.65)\n",
      "\n",
      "Iteration 125, loss = 3.8134\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 187 / 4600 correct (4.07)\n",
      "\n",
      "Iteration 150, loss = 3.8948\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 175, loss = 3.9183\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 189 / 4600 correct (4.11)\n",
      "\n",
      "Iteration 200, loss = 3.8171\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 163 / 4600 correct (3.54)\n",
      "\n",
      "Iteration 225, loss = 3.8637\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 206 / 4600 correct (4.48)\n",
      "\n",
      "Iteration 250, loss = 3.8630\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 191 / 4600 correct (4.15)\n",
      "\n",
      "Iteration 275, loss = 3.8810\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 185 / 4600 correct (4.02)\n",
      "\n",
      "Iteration 300, loss = 3.8372\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 164 / 4600 correct (3.57)\n",
      "\n",
      "Iteration 325, loss = 3.9401\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 188 / 4600 correct (4.09)\n",
      "\n",
      "Iteration 350, loss = 3.8934\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 172 / 4600 correct (3.74)\n",
      "\n",
      "Iteration 0, loss = 3.8607\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 175 / 4600 correct (3.80)\n",
      "\n",
      "Iteration 25, loss = 3.8883\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 190 / 4600 correct (4.13)\n",
      "\n",
      "Iteration 50, loss = 3.7983\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 166 / 4600 correct (3.61)\n",
      "\n",
      "Iteration 75, loss = 3.9259\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 178 / 4600 correct (3.87)\n",
      "\n",
      "Iteration 100, loss = 3.8282\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 190 / 4600 correct (4.13)\n",
      "\n",
      "Iteration 125, loss = 3.8885\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 171 / 4600 correct (3.72)\n",
      "\n",
      "Iteration 150, loss = 3.8345\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 181 / 4600 correct (3.93)\n",
      "\n",
      "Iteration 175, loss = 3.8271\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 190 / 4600 correct (4.13)\n",
      "\n",
      "Iteration 200, loss = 3.8350\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 167 / 4600 correct (3.63)\n",
      "\n",
      "Iteration 225, loss = 3.8402\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 195 / 4600 correct (4.24)\n",
      "\n",
      "Iteration 250, loss = 3.8117\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 275, loss = 3.8452\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 178 / 4600 correct (3.87)\n",
      "\n",
      "Iteration 300, loss = 3.8494\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 180 / 4600 correct (3.91)\n",
      "\n",
      "Iteration 325, loss = 3.7779\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 173 / 4600 correct (3.76)\n",
      "\n",
      "Iteration 350, loss = 3.7547\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 157 / 4600 correct (3.41)\n",
      "\n",
      "Iteration 0, loss = 3.8233\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 185 / 4600 correct (4.02)\n",
      "\n",
      "Iteration 25, loss = 3.8554\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 181 / 4600 correct (3.93)\n",
      "\n",
      "Iteration 50, loss = 3.8636\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 75, loss = 3.8729\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 183 / 4600 correct (3.98)\n",
      "\n",
      "Iteration 100, loss = 3.8365\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 196 / 4600 correct (4.26)\n",
      "\n",
      "Iteration 125, loss = 3.8353\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 182 / 4600 correct (3.96)\n",
      "\n",
      "Iteration 150, loss = 3.8072\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 200 / 4600 correct (4.35)\n",
      "\n",
      "Iteration 175, loss = 3.8049\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 176 / 4600 correct (3.83)\n",
      "\n",
      "Iteration 200, loss = 3.8418\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 190 / 4600 correct (4.13)\n",
      "\n",
      "Iteration 225, loss = 3.7843\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 219 / 4600 correct (4.76)\n",
      "\n",
      "Iteration 250, loss = 3.9152\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 179 / 4600 correct (3.89)\n",
      "\n",
      "Iteration 275, loss = 3.8690\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 162 / 4600 correct (3.52)\n",
      "\n",
      "Iteration 300, loss = 3.8473\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 182 / 4600 correct (3.96)\n",
      "\n",
      "Iteration 325, loss = 3.8370\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 188 / 4600 correct (4.09)\n",
      "\n",
      "Iteration 350, loss = 3.8753\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 192 / 4600 correct (4.17)\n",
      "\n",
      "Iteration 0, loss = 3.8222\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 177 / 4600 correct (3.85)\n",
      "\n",
      "Iteration 25, loss = 3.8444\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 202 / 4600 correct (4.39)\n",
      "\n",
      "Iteration 50, loss = 3.8661\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 184 / 4600 correct (4.00)\n",
      "\n",
      "Iteration 75, loss = 3.8319\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 174 / 4600 correct (3.78)\n",
      "\n",
      "Iteration 100, loss = 3.7846\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 181 / 4600 correct (3.93)\n",
      "\n",
      "Iteration 125, loss = 3.8510\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 185 / 4600 correct (4.02)\n",
      "\n",
      "Iteration 150, loss = 3.8703\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 200 / 4600 correct (4.35)\n",
      "\n",
      "Iteration 175, loss = 3.8244\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 170 / 4600 correct (3.70)\n",
      "\n",
      "Iteration 200, loss = 3.8159\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 195 / 4600 correct (4.24)\n",
      "\n",
      "Iteration 225, loss = 3.8115\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 181 / 4600 correct (3.93)\n",
      "\n",
      "Iteration 250, loss = 3.8287\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 170 / 4600 correct (3.70)\n",
      "\n",
      "Iteration 275, loss = 3.8515\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 188 / 4600 correct (4.09)\n",
      "\n",
      "Iteration 300, loss = 3.8101\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 194 / 4600 correct (4.22)\n",
      "\n",
      "Iteration 325, loss = 3.8431\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 184 / 4600 correct (4.00)\n",
      "\n",
      "Iteration 350, loss = 3.8731\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 175 / 4600 correct (3.80)\n",
      "\n",
      "Test accuracy\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "Got 199 / 4600 correct (4.33)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "----------\n",
      "Epoch 0\n",
      "----------\n",
      "Iteration 0, training loss = 3.3333\n",
      "Loss: 2.8040989948355635\n",
      "Got 709 / 4600 correct (15.413043478260867%)\n",
      "\n",
      "Iteration 50, training loss = 2.3445\n",
      "Loss: 2.248286387194758\n",
      "Got 839 / 4600 correct (18.23913043478261%)\n",
      "\n",
      "Iteration 100, training loss = 2.4003\n",
      "Loss: 2.195775975351748\n",
      "Got 970 / 4600 correct (21.086956521739133%)\n",
      "\n",
      "Iteration 150, training loss = 2.3846\n",
      "Loss: 2.23553234597911\n",
      "Got 902 / 4600 correct (19.608695652173914%)\n",
      "\n",
      "Iteration 200, training loss = 2.3251\n",
      "Loss: 2.1915271282196045\n",
      "Got 828 / 4600 correct (18.0%)\n",
      "\n",
      "Iteration 250, training loss = 2.3030\n",
      "Loss: 2.2256835906401924\n",
      "Got 778 / 4600 correct (16.91304347826087%)\n",
      "\n",
      "Iteration 300, training loss = 2.2628\n",
      "Loss: 2.195758658906688\n",
      "Got 801 / 4600 correct (17.41304347826087%)\n",
      "\n",
      "Iteration 350, training loss = 2.2706\n",
      "Loss: 2.1902600267659063\n",
      "Got 901 / 4600 correct (19.58695652173913%)\n",
      "\n",
      "Loss: 2.2255973867748096\n",
      "Got 675 / 4600 correct (14.673913043478262%)\n",
      "******************************\n",
      "End of epoch 0 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.3329432834831536, accuracy: 0.17044145456126242\n",
      "Val loss: 2.2255973867748096, accuracy: 0.14673913043478262\n",
      "******************************\n",
      "----------\n",
      "Epoch 1\n",
      "----------\n",
      "Iteration 0, training loss = 2.2634\n",
      "Loss: 2.2580581229666006\n",
      "Got 608 / 4600 correct (13.217391304347824%)\n",
      "\n",
      "Iteration 50, training loss = 2.2577\n",
      "Loss: 2.189487161843673\n",
      "Got 912 / 4600 correct (19.82608695652174%)\n",
      "\n",
      "Iteration 100, training loss = 2.2837\n",
      "Loss: 2.2381134810655015\n",
      "Got 822 / 4600 correct (17.869565217391305%)\n",
      "\n",
      "Iteration 150, training loss = 2.2849\n",
      "Loss: 2.181565310644067\n",
      "Got 1034 / 4600 correct (22.47826086956522%)\n",
      "\n",
      "Iteration 200, training loss = 2.3395\n",
      "Loss: 2.1901344579199087\n",
      "Got 852 / 4600 correct (18.521739130434785%)\n",
      "\n",
      "Iteration 250, training loss = 2.2956\n",
      "Loss: 2.181563190791918\n",
      "Got 885 / 4600 correct (19.23913043478261%)\n",
      "\n",
      "Iteration 300, training loss = 2.3044\n",
      "Loss: 2.222470558207968\n",
      "Got 766 / 4600 correct (16.652173913043477%)\n",
      "\n",
      "Iteration 350, training loss = 2.2715\n",
      "Loss: 2.2143122372419937\n",
      "Got 754 / 4600 correct (16.391304347826086%)\n",
      "\n",
      "Loss: 2.1769543886184692\n",
      "Got 977 / 4600 correct (21.23913043478261%)\n",
      "******************************\n",
      "End of epoch 1 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.27859656267197, accuracy: 0.1828526093855283\n",
      "Val loss: 2.1769543886184692, accuracy: 0.21239130434782608\n",
      "******************************\n",
      "----------\n",
      "Epoch 2\n",
      "----------\n",
      "Iteration 0, training loss = 2.2707\n",
      "Loss: 2.1838170341823413\n",
      "Got 952 / 4600 correct (20.695652173913043%)\n",
      "\n",
      "Iteration 50, training loss = 2.3110\n",
      "Loss: 2.1760673678439595\n",
      "Got 887 / 4600 correct (19.282608695652172%)\n",
      "\n",
      "Iteration 100, training loss = 2.3183\n",
      "Loss: 2.194360183632892\n",
      "Got 851 / 4600 correct (18.5%)\n",
      "\n",
      "Iteration 150, training loss = 2.2726\n",
      "Loss: 2.202416067538054\n",
      "Got 854 / 4600 correct (18.565217391304348%)\n",
      "\n",
      "Iteration 200, training loss = 2.2867\n",
      "Loss: 2.1658767461776733\n",
      "Got 1032 / 4600 correct (22.434782608695652%)\n",
      "\n",
      "Iteration 250, training loss = 2.2605\n",
      "Loss: 2.1865377011506455\n",
      "Got 842 / 4600 correct (18.304347826086957%)\n",
      "\n",
      "Iteration 300, training loss = 2.2189\n",
      "Loss: 2.1963367410328076\n",
      "Got 871 / 4600 correct (18.934782608695652%)\n",
      "\n",
      "Iteration 350, training loss = 2.2970\n",
      "Loss: 2.1732581905696704\n",
      "Got 900 / 4600 correct (19.565217391304348%)\n",
      "\n",
      "Loss: 2.1789488999739937\n",
      "Got 913 / 4600 correct (19.847826086956523%)\n",
      "******************************\n",
      "End of epoch 2 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.269183809046698, accuracy: 0.18634120894646466\n",
      "Val loss: 2.1789488999739937, accuracy: 0.19847826086956522\n",
      "******************************\n",
      "----------\n",
      "Epoch 3\n",
      "----------\n",
      "Iteration 0, training loss = 2.2529\n",
      "Loss: 2.1878441675849585\n",
      "Got 885 / 4600 correct (19.23913043478261%)\n",
      "\n",
      "Iteration 50, training loss = 2.2306\n",
      "Loss: 2.1613688209782476\n",
      "Got 1054 / 4600 correct (22.91304347826087%)\n",
      "\n",
      "Iteration 100, training loss = 2.3130\n",
      "Loss: 2.192023111426312\n",
      "Got 894 / 4600 correct (19.434782608695652%)\n",
      "\n",
      "Iteration 150, training loss = 2.2457\n",
      "Loss: 2.168906103009763\n",
      "Got 882 / 4600 correct (19.17391304347826%)\n",
      "\n",
      "Iteration 200, training loss = 2.1862\n",
      "Loss: 2.2097667352012964\n",
      "Got 815 / 4600 correct (17.717391304347828%)\n",
      "\n",
      "Iteration 250, training loss = 2.2540\n",
      "Loss: 2.2127207724944404\n",
      "Got 858 / 4600 correct (18.65217391304348%)\n",
      "\n",
      "Iteration 300, training loss = 2.2660\n",
      "Loss: 2.2042917635129844\n",
      "Got 826 / 4600 correct (17.956521739130434%)\n",
      "\n",
      "Iteration 350, training loss = 2.2248\n",
      "Loss: 2.1908441574677178\n",
      "Got 817 / 4600 correct (17.76086956521739%)\n",
      "\n",
      "Loss: 2.2069765536681465\n",
      "Got 863 / 4600 correct (18.76086956521739%)\n",
      "******************************\n",
      "End of epoch 3 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.2640116375245674, accuracy: 0.187824678853218\n",
      "Val loss: 2.2069765536681465, accuracy: 0.18760869565217392\n",
      "******************************\n",
      "----------\n",
      "Epoch 4\n",
      "----------\n",
      "Iteration 0, training loss = 2.3248\n",
      "Loss: 2.2454766864361972\n",
      "Got 739 / 4600 correct (16.065217391304348%)\n",
      "\n",
      "Iteration 50, training loss = 2.2952\n",
      "Loss: 2.2107799364172895\n",
      "Got 845 / 4600 correct (18.369565217391305%)\n",
      "\n",
      "Iteration 100, training loss = 2.2926\n",
      "Loss: 2.1849184243575386\n",
      "Got 836 / 4600 correct (18.173913043478258%)\n",
      "\n",
      "Iteration 150, training loss = 2.2658\n",
      "Loss: 2.1780186787895532\n",
      "Got 923 / 4600 correct (20.065217391304348%)\n",
      "\n",
      "Iteration 200, training loss = 2.1770\n",
      "Loss: 2.1743552477463433\n",
      "Got 867 / 4600 correct (18.84782608695652%)\n",
      "\n",
      "Iteration 250, training loss = 2.2939\n",
      "Loss: 2.205377760140792\n",
      "Got 840 / 4600 correct (18.26086956521739%)\n",
      "\n",
      "Iteration 300, training loss = 2.2456\n",
      "Loss: 2.181050046630528\n",
      "Got 933 / 4600 correct (20.282608695652176%)\n",
      "\n",
      "Iteration 350, training loss = 2.2566\n",
      "Loss: 2.216110913649849\n",
      "Got 801 / 4600 correct (17.41304347826087%)\n",
      "\n",
      "Loss: 2.2042164958041646\n",
      "Got 830 / 4600 correct (18.043478260869566%)\n",
      "******************************\n",
      "End of epoch 4 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.2594726976487145, accuracy: 0.18794422587867066\n",
      "Val loss: 2.2042164958041646, accuracy: 0.18043478260869567\n",
      "******************************\n",
      "----------\n",
      "Epoch 5\n",
      "----------\n",
      "Iteration 0, training loss = 2.2955\n",
      "Loss: 2.2181831650111987\n",
      "Got 803 / 4600 correct (17.456521739130434%)\n",
      "\n",
      "Iteration 50, training loss = 2.3016\n",
      "Loss: 2.218600542648979\n",
      "Got 761 / 4600 correct (16.543478260869566%)\n",
      "\n",
      "Iteration 100, training loss = 2.2341\n",
      "Loss: 2.1609627319418867\n",
      "Got 987 / 4600 correct (21.456521739130434%)\n",
      "\n",
      "Iteration 150, training loss = 2.3073\n",
      "Loss: 2.185492437818776\n",
      "Got 874 / 4600 correct (19.0%)\n",
      "\n",
      "Iteration 200, training loss = 2.2640\n",
      "Loss: 2.210694328598354\n",
      "Got 857 / 4600 correct (18.630434782608695%)\n",
      "\n",
      "Iteration 250, training loss = 2.3309\n",
      "Loss: 2.2123176740563433\n",
      "Got 773 / 4600 correct (16.804347826086957%)\n",
      "\n",
      "Iteration 300, training loss = 2.2271\n",
      "Loss: 2.201415077499721\n",
      "Got 870 / 4600 correct (18.913043478260867%)\n",
      "\n",
      "Iteration 350, training loss = 2.2408\n",
      "Loss: 2.191649405852608\n",
      "Got 1001 / 4600 correct (21.76086956521739%)\n",
      "\n",
      "Loss: 2.185974084812662\n",
      "Got 869 / 4600 correct (18.891304347826086%)\n",
      "******************************\n",
      "End of epoch 5 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.258948331344886, accuracy: 0.1885310930945291\n",
      "Val loss: 2.185974084812662, accuracy: 0.18891304347826088\n",
      "******************************\n",
      "----------\n",
      "Epoch 6\n",
      "----------\n",
      "Iteration 0, training loss = 2.1703\n",
      "Loss: 2.186723926792974\n",
      "Got 871 / 4600 correct (18.934782608695652%)\n",
      "\n",
      "Iteration 50, training loss = 2.1961\n",
      "Loss: 2.1973465266435044\n",
      "Got 996 / 4600 correct (21.65217391304348%)\n",
      "\n",
      "Iteration 100, training loss = 2.2456\n",
      "Loss: 2.156582728676174\n",
      "Got 911 / 4600 correct (19.804347826086957%)\n",
      "\n",
      "Iteration 150, training loss = 2.1913\n",
      "Loss: 2.184723698574564\n",
      "Got 910 / 4600 correct (19.782608695652172%)\n",
      "\n",
      "Iteration 200, training loss = 2.2792\n",
      "Loss: 2.236222635144773\n",
      "Got 799 / 4600 correct (17.369565217391305%)\n",
      "\n",
      "Iteration 250, training loss = 2.2035\n",
      "Loss: 2.201775177665379\n",
      "Got 840 / 4600 correct (18.26086956521739%)\n",
      "\n",
      "Iteration 300, training loss = 2.2820\n",
      "Loss: 2.1767896206482598\n",
      "Got 881 / 4600 correct (19.152173913043477%)\n",
      "\n",
      "Iteration 350, training loss = 2.2818\n",
      "Loss: 2.207130888234014\n",
      "Got 851 / 4600 correct (18.5%)\n",
      "\n",
      "Loss: 2.1998414526815\n",
      "Got 855 / 4600 correct (18.586956521739133%)\n",
      "******************************\n",
      "End of epoch 6 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.255625804578001, accuracy: 0.1908133544895342\n",
      "Val loss: 2.1998414526815, accuracy: 0.1858695652173913\n",
      "******************************\n",
      "----------\n",
      "Epoch 7\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss = 2.2403\n",
      "Loss: 2.2016958361086636\n",
      "Got 818 / 4600 correct (17.782608695652176%)\n",
      "\n",
      "Iteration 50, training loss = 2.2758\n",
      "Loss: 2.2052086280739824\n",
      "Got 785 / 4600 correct (17.065217391304348%)\n",
      "\n",
      "Iteration 100, training loss = 2.2296\n",
      "Loss: 2.177368428396142\n",
      "Got 833 / 4600 correct (18.10869565217391%)\n",
      "\n",
      "Iteration 150, training loss = 2.2415\n",
      "Loss: 2.2217489791953047\n",
      "Got 789 / 4600 correct (17.152173913043477%)\n",
      "\n",
      "Iteration 200, training loss = 2.2472\n",
      "Loss: 2.218584620434305\n",
      "Got 883 / 4600 correct (19.195652173913043%)\n",
      "\n",
      "Iteration 250, training loss = 2.2188\n",
      "Loss: 2.20131880822389\n",
      "Got 851 / 4600 correct (18.5%)\n",
      "\n",
      "Iteration 300, training loss = 2.1636\n",
      "Loss: 2.1706973936246787\n",
      "Got 896 / 4600 correct (19.47826086956522%)\n",
      "\n",
      "Iteration 350, training loss = 2.1984\n",
      "Loss: 2.21216563038204\n",
      "Got 791 / 4600 correct (17.195652173913043%)\n",
      "\n",
      "Loss: 2.2135827593181445\n",
      "Got 813 / 4600 correct (17.67391304347826%)\n",
      "******************************\n",
      "End of epoch 7 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.253305295556003, accuracy: 0.19085139217945096\n",
      "Val loss: 2.2135827593181445, accuracy: 0.17673913043478262\n",
      "******************************\n",
      "----------\n",
      "Epoch 8\n",
      "----------\n",
      "Iteration 0, training loss = 2.2346\n",
      "Loss: 2.215766668319702\n",
      "Got 813 / 4600 correct (17.67391304347826%)\n",
      "\n",
      "Iteration 50, training loss = 2.2270\n",
      "Loss: 2.1876296686089556\n",
      "Got 893 / 4600 correct (19.413043478260867%)\n",
      "\n",
      "Iteration 100, training loss = 2.1415\n",
      "Loss: 2.189150649568309\n",
      "Got 848 / 4600 correct (18.434782608695652%)\n",
      "\n",
      "Iteration 150, training loss = 2.1883\n",
      "Loss: 2.1772892319637798\n",
      "Got 939 / 4600 correct (20.41304347826087%)\n",
      "\n",
      "Iteration 200, training loss = 2.1998\n",
      "Loss: 2.1874827094700025\n",
      "Got 886 / 4600 correct (19.26086956521739%)\n",
      "\n",
      "Iteration 250, training loss = 2.2162\n",
      "Loss: 2.186399547950081\n",
      "Got 889 / 4600 correct (19.326086956521742%)\n",
      "\n",
      "Iteration 300, training loss = 2.2612\n",
      "Loss: 2.1780504454737124\n",
      "Got 917 / 4600 correct (19.934782608695652%)\n",
      "\n",
      "Iteration 350, training loss = 2.2897\n",
      "Loss: 2.1809738252473916\n",
      "Got 897 / 4600 correct (19.5%)\n",
      "\n",
      "Loss: 2.189760192580845\n",
      "Got 876 / 4600 correct (19.043478260869566%)\n",
      "******************************\n",
      "End of epoch 8 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.23595876400619, accuracy: 0.20065968222227054\n",
      "Val loss: 2.189760192580845, accuracy: 0.19043478260869565\n",
      "******************************\n",
      "----------\n",
      "Epoch 9\n",
      "----------\n",
      "Iteration 0, training loss = 2.2225\n",
      "Loss: 2.190845328828563\n",
      "Got 869 / 4600 correct (18.891304347826086%)\n",
      "\n",
      "Iteration 50, training loss = 2.2672\n",
      "Loss: 2.1758179405461187\n",
      "Got 888 / 4600 correct (19.304347826086957%)\n",
      "\n",
      "Iteration 100, training loss = 2.2161\n",
      "Loss: 2.186368273652118\n",
      "Got 867 / 4600 correct (18.84782608695652%)\n",
      "\n",
      "Iteration 150, training loss = 2.2861\n",
      "Loss: 2.1825203791908594\n",
      "Got 897 / 4600 correct (19.5%)\n",
      "\n",
      "Iteration 200, training loss = 2.2002\n",
      "Loss: 2.184500575065613\n",
      "Got 863 / 4600 correct (18.76086956521739%)\n",
      "\n",
      "Iteration 250, training loss = 2.2258\n",
      "Loss: 2.186556064564249\n",
      "Got 899 / 4600 correct (19.543478260869566%)\n",
      "\n",
      "Iteration 300, training loss = 2.2572\n",
      "Loss: 2.1841742318609487\n",
      "Got 896 / 4600 correct (19.47826086956522%)\n",
      "\n",
      "Iteration 350, training loss = 2.2936\n",
      "Loss: 2.1811155910077304\n",
      "Got 905 / 4600 correct (19.67391304347826%)\n",
      "\n",
      "Loss: 2.1786273510559746\n",
      "Got 889 / 4600 correct (19.326086956521742%)\n",
      "******************************\n",
      "End of epoch 9 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.2341799316418274, accuracy: 0.20141500206490318\n",
      "Val loss: 2.1786273510559746, accuracy: 0.1932608695652174\n",
      "******************************\n",
      "----------\n",
      "Epoch 10\n",
      "----------\n",
      "Iteration 0, training loss = 2.2311\n",
      "Loss: 2.1792094085527505\n",
      "Got 893 / 4600 correct (19.413043478260867%)\n",
      "\n",
      "Iteration 50, training loss = 2.2251\n",
      "Loss: 2.1776976844538813\n",
      "Got 837 / 4600 correct (18.195652173913043%)\n",
      "\n",
      "Iteration 100, training loss = 2.2358\n",
      "Loss: 2.171914572301118\n",
      "Got 881 / 4600 correct (19.152173913043477%)\n",
      "\n",
      "Iteration 150, training loss = 2.2230\n",
      "Loss: 2.178935496703438\n",
      "Got 887 / 4600 correct (19.282608695652172%)\n",
      "\n",
      "Iteration 200, training loss = 2.1906\n",
      "Loss: 2.188335361688033\n",
      "Got 859 / 4600 correct (18.67391304347826%)\n",
      "\n",
      "Iteration 250, training loss = 2.2514\n",
      "Loss: 2.186138883880947\n",
      "Got 922 / 4600 correct (20.043478260869566%)\n",
      "\n",
      "Iteration 300, training loss = 2.1711\n",
      "Loss: 2.1769581618516343\n",
      "Got 859 / 4600 correct (18.67391304347826%)\n",
      "\n",
      "Iteration 350, training loss = 2.2272\n",
      "Loss: 2.177137499270232\n",
      "Got 894 / 4600 correct (19.434782608695652%)\n",
      "\n",
      "Loss: 2.185113471487294\n",
      "Got 874 / 4600 correct (19.0%)\n",
      "******************************\n",
      "End of epoch 10 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.233809365480371, accuracy: 0.20015975829765037\n",
      "Val loss: 2.185113471487294, accuracy: 0.19\n",
      "******************************\n",
      "----------\n",
      "Epoch 11\n",
      "----------\n",
      "Iteration 0, training loss = 2.1776\n",
      "Loss: 2.1862340025279834\n",
      "Got 879 / 4600 correct (19.108695652173914%)\n",
      "\n",
      "Iteration 50, training loss = 2.2062\n",
      "Loss: 2.182214125342991\n",
      "Got 887 / 4600 correct (19.282608695652172%)\n",
      "\n",
      "Iteration 100, training loss = 2.2357\n",
      "Loss: 2.1867407664008764\n",
      "Got 892 / 4600 correct (19.39130434782609%)\n",
      "\n",
      "Iteration 150, training loss = 2.2294\n",
      "Loss: 2.175799038099206\n",
      "Got 926 / 4600 correct (20.130434782608695%)\n",
      "\n",
      "Iteration 200, training loss = 2.2576\n",
      "Loss: 2.1781352240106333\n",
      "Got 875 / 4600 correct (19.021739130434785%)\n",
      "\n",
      "Iteration 250, training loss = 2.2515\n",
      "Loss: 2.175944411236307\n",
      "Got 903 / 4600 correct (19.630434782608695%)\n",
      "\n",
      "Iteration 300, training loss = 2.2248\n",
      "Loss: 2.185223206229832\n",
      "Got 940 / 4600 correct (20.434782608695652%)\n",
      "\n",
      "Iteration 350, training loss = 2.2250\n",
      "Loss: 2.1787822505702144\n",
      "Got 887 / 4600 correct (19.282608695652172%)\n",
      "\n",
      "Loss: 2.1792417920154072\n",
      "Got 908 / 4600 correct (19.73913043478261%)\n",
      "******************************\n",
      "End of epoch 11 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.233473152191054, accuracy: 0.20090421022887822\n",
      "Val loss: 2.1792417920154072, accuracy: 0.1973913043478261\n",
      "******************************\n",
      "----------\n",
      "Epoch 12\n",
      "----------\n",
      "Iteration 0, training loss = 2.2134\n",
      "Loss: 2.1816454141036323\n",
      "Got 900 / 4600 correct (19.565217391304348%)\n",
      "\n",
      "Iteration 50, training loss = 2.2371\n",
      "Loss: 2.1796967413114463\n",
      "Got 947 / 4600 correct (20.58695652173913%)\n",
      "\n",
      "Iteration 100, training loss = 2.2858\n",
      "Loss: 2.175375326820042\n",
      "Got 934 / 4600 correct (20.304347826086957%)\n",
      "\n",
      "Iteration 150, training loss = 2.2590\n",
      "Loss: 2.1851700492527173\n",
      "Got 913 / 4600 correct (19.847826086956523%)\n",
      "\n",
      "Iteration 200, training loss = 2.2742\n",
      "Loss: 2.1838939863702524\n",
      "Got 896 / 4600 correct (19.47826086956522%)\n",
      "\n",
      "Iteration 250, training loss = 2.2668\n",
      "Loss: 2.182496993438057\n",
      "Got 901 / 4600 correct (19.58695652173913%)\n",
      "\n",
      "Iteration 300, training loss = 2.2105\n",
      "Loss: 2.185203650723333\n",
      "Got 853 / 4600 correct (18.543478260869563%)\n",
      "\n",
      "Iteration 350, training loss = 2.2346\n",
      "Loss: 2.1815771071807197\n",
      "Got 921 / 4600 correct (20.02173913043478%)\n",
      "\n",
      "Loss: 2.1970558425654536\n",
      "Got 846 / 4600 correct (18.391304347826086%)\n",
      "******************************\n",
      "End of epoch 12 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.233655327204442, accuracy: 0.2016323602929989\n",
      "Val loss: 2.1970558425654536, accuracy: 0.18391304347826087\n",
      "******************************\n",
      "----------\n",
      "Epoch 13\n",
      "----------\n",
      "Iteration 0, training loss = 2.1913\n",
      "Loss: 2.1980386298635732\n",
      "Got 858 / 4600 correct (18.65217391304348%)\n",
      "\n",
      "Iteration 50, training loss = 2.1241\n",
      "Loss: 2.1910322230795156\n",
      "Got 864 / 4600 correct (18.782608695652172%)\n",
      "\n",
      "Iteration 100, training loss = 2.2035\n",
      "Loss: 2.183664638063182\n",
      "Got 875 / 4600 correct (19.021739130434785%)\n",
      "\n",
      "Iteration 150, training loss = 2.2599\n",
      "Loss: 2.1844331751699033\n",
      "Got 905 / 4600 correct (19.67391304347826%)\n",
      "\n",
      "Iteration 200, training loss = 2.2388\n",
      "Loss: 2.184817013533219\n",
      "Got 922 / 4600 correct (20.043478260869566%)\n",
      "\n",
      "Iteration 250, training loss = 2.2147\n",
      "Loss: 2.181212228277455\n",
      "Got 906 / 4600 correct (19.695652173913043%)\n",
      "\n",
      "Iteration 300, training loss = 2.2351\n",
      "Loss: 2.179314240165379\n",
      "Got 883 / 4600 correct (19.195652173913043%)\n",
      "\n",
      "Iteration 350, training loss = 2.3000\n",
      "Loss: 2.186940846235856\n",
      "Got 887 / 4600 correct (19.282608695652172%)\n",
      "\n",
      "Loss: 2.189881506173507\n",
      "Got 882 / 4600 correct (19.17391304347826%)\n",
      "******************************\n",
      "End of epoch 13 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.233915419321203, accuracy: 0.20145303975481993\n",
      "Val loss: 2.189881506173507, accuracy: 0.1917391304347826\n",
      "******************************\n",
      "----------\n",
      "Epoch 14\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss = 2.1908\n",
      "Loss: 2.1892996663632602\n",
      "Got 884 / 4600 correct (19.217391304347824%)\n",
      "\n",
      "Iteration 50, training loss = 2.2627\n",
      "Loss: 2.1822831682536914\n",
      "Got 905 / 4600 correct (19.67391304347826%)\n",
      "\n",
      "Iteration 100, training loss = 2.1968\n",
      "Loss: 2.1825777914213096\n",
      "Got 896 / 4600 correct (19.47826086956522%)\n",
      "\n",
      "Iteration 150, training loss = 2.2035\n",
      "Loss: 2.185186391291411\n",
      "Got 863 / 4600 correct (18.76086956521739%)\n",
      "\n",
      "Iteration 200, training loss = 2.2646\n",
      "Loss: 2.1819343463234278\n",
      "Got 897 / 4600 correct (19.5%)\n",
      "\n",
      "Iteration 250, training loss = 2.2355\n",
      "Loss: 2.184503949206808\n",
      "Got 897 / 4600 correct (19.5%)\n",
      "\n",
      "Iteration 300, training loss = 2.1956\n",
      "Loss: 2.1803825834523076\n",
      "Got 914 / 4600 correct (19.869565217391305%)\n",
      "\n",
      "Iteration 350, training loss = 2.2068\n",
      "Loss: 2.178369905637658\n",
      "Got 918 / 4600 correct (19.956521739130434%)\n",
      "\n",
      "Loss: 2.1799672634705254\n",
      "Got 919 / 4600 correct (19.97826086956522%)\n",
      "******************************\n",
      "End of epoch 14 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.2309965705048644, accuracy: 0.20235507640141717\n",
      "Val loss: 2.1799672634705254, accuracy: 0.19978260869565218\n",
      "******************************\n",
      "----------\n",
      "Epoch 15\n",
      "----------\n",
      "Iteration 0, training loss = 2.2044\n",
      "Loss: 2.1793806138245957\n",
      "Got 921 / 4600 correct (20.02173913043478%)\n",
      "\n",
      "Iteration 50, training loss = 2.2224\n",
      "Loss: 2.184447863827581\n",
      "Got 898 / 4600 correct (19.52173913043478%)\n",
      "\n",
      "Iteration 100, training loss = 2.1838\n",
      "Loss: 2.1788893212442813\n",
      "Got 921 / 4600 correct (20.02173913043478%)\n",
      "\n",
      "Iteration 150, training loss = 2.2467\n",
      "Loss: 2.176714477331742\n",
      "Got 925 / 4600 correct (20.108695652173914%)\n",
      "\n",
      "Iteration 200, training loss = 2.2408\n",
      "Loss: 2.179874212845512\n",
      "Got 881 / 4600 correct (19.152173913043477%)\n",
      "\n",
      "Iteration 250, training loss = 2.2633\n",
      "Loss: 2.1805359539778335\n",
      "Got 898 / 4600 correct (19.52173913043478%)\n",
      "\n",
      "Iteration 300, training loss = 2.2411\n",
      "Loss: 2.1813426691552866\n",
      "Got 899 / 4600 correct (19.543478260869566%)\n",
      "\n",
      "Iteration 350, training loss = 2.2051\n",
      "Loss: 2.1834215755048008\n",
      "Got 909 / 4600 correct (19.760869565217394%)\n",
      "\n",
      "Loss: 2.1801821459894595\n",
      "Got 895 / 4600 correct (19.456521739130434%)\n",
      "******************************\n",
      "End of epoch 15 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.231624389022798, accuracy: 0.20272458538917992\n",
      "Val loss: 2.1801821459894595, accuracy: 0.19456521739130433\n",
      "******************************\n",
      "----------\n",
      "Epoch 16\n",
      "----------\n",
      "Iteration 0, training loss = 2.2215\n",
      "Loss: 2.1805053534715073\n",
      "Got 895 / 4600 correct (19.456521739130434%)\n",
      "\n",
      "Iteration 50, training loss = 2.2674\n",
      "Loss: 2.179715809614762\n",
      "Got 898 / 4600 correct (19.52173913043478%)\n",
      "\n",
      "Iteration 100, training loss = 2.1995\n",
      "Loss: 2.183789004450259\n",
      "Got 890 / 4600 correct (19.34782608695652%)\n",
      "\n",
      "Iteration 150, training loss = 2.2910\n",
      "Loss: 2.1802805765815405\n",
      "Got 900 / 4600 correct (19.565217391304348%)\n",
      "\n",
      "Iteration 200, training loss = 2.2315\n",
      "Loss: 2.179133062777312\n",
      "Got 925 / 4600 correct (20.108695652173914%)\n",
      "\n",
      "Iteration 250, training loss = 2.2621\n",
      "Loss: 2.1792511784512065\n",
      "Got 922 / 4600 correct (20.043478260869566%)\n",
      "\n",
      "Iteration 300, training loss = 2.1741\n",
      "Loss: 2.179847105689671\n",
      "Got 920 / 4600 correct (20.0%)\n",
      "\n",
      "Iteration 350, training loss = 2.2462\n",
      "Loss: 2.1848570885865586\n",
      "Got 877 / 4600 correct (19.065217391304348%)\n",
      "\n",
      "Loss: 2.1877280473709106\n",
      "Got 872 / 4600 correct (18.956521739130437%)\n",
      "******************************\n",
      "End of epoch 16 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.2314328428460755, accuracy: 0.202822396591823\n",
      "Val loss: 2.1877280473709106, accuracy: 0.18956521739130436\n",
      "******************************\n",
      "----------\n",
      "Epoch 17\n",
      "----------\n",
      "Iteration 0, training loss = 2.2497\n",
      "Loss: 2.186788833659628\n",
      "Got 869 / 4600 correct (18.891304347826086%)\n",
      "\n",
      "Iteration 50, training loss = 2.2353\n",
      "Loss: 2.178977131843567\n",
      "Got 925 / 4600 correct (20.108695652173914%)\n",
      "\n",
      "Iteration 100, training loss = 2.2142\n",
      "Loss: 2.180925457373909\n",
      "Got 891 / 4600 correct (19.369565217391305%)\n",
      "\n",
      "Iteration 150, training loss = 2.2407\n",
      "Loss: 2.181712674058002\n",
      "Got 907 / 4600 correct (19.717391304347824%)\n",
      "\n",
      "Iteration 200, training loss = 2.1611\n",
      "Loss: 2.1797349401142285\n",
      "Got 931 / 4600 correct (20.23913043478261%)\n",
      "\n",
      "Iteration 250, training loss = 2.2878\n",
      "Loss: 2.1814970296362173\n",
      "Got 892 / 4600 correct (19.39130434782609%)\n",
      "\n",
      "Iteration 300, training loss = 2.2662\n",
      "Loss: 2.179438077885172\n",
      "Got 896 / 4600 correct (19.47826086956522%)\n",
      "\n",
      "Iteration 350, training loss = 2.2894\n",
      "Loss: 2.180682094200798\n",
      "Got 880 / 4600 correct (19.130434782608695%)\n",
      "\n",
      "Loss: 2.183288361715234\n",
      "Got 891 / 4600 correct (19.369565217391305%)\n",
      "******************************\n",
      "End of epoch 17 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.2310685737552736, accuracy: 0.20332775447214554\n",
      "Val loss: 2.183288361715234, accuracy: 0.19369565217391305\n",
      "******************************\n",
      "----------\n",
      "Epoch 18\n",
      "----------\n",
      "Iteration 0, training loss = 2.2198\n",
      "Loss: 2.1827463533567344\n",
      "Got 889 / 4600 correct (19.326086956521742%)\n",
      "\n",
      "Iteration 50, training loss = 2.2276\n",
      "Loss: 2.1783617786739184\n",
      "Got 908 / 4600 correct (19.73913043478261%)\n",
      "\n",
      "Iteration 100, training loss = 2.2496\n",
      "Loss: 2.1807554597439975\n",
      "Got 901 / 4600 correct (19.58695652173913%)\n",
      "\n",
      "Iteration 150, training loss = 2.2343\n",
      "Loss: 2.1825463823650195\n",
      "Got 884 / 4600 correct (19.217391304347824%)\n",
      "\n",
      "Iteration 200, training loss = 2.2258\n",
      "Loss: 2.1796394431072734\n",
      "Got 885 / 4600 correct (19.23913043478261%)\n",
      "\n",
      "Iteration 250, training loss = 2.2263\n",
      "Loss: 2.1796409669129746\n",
      "Got 935 / 4600 correct (20.32608695652174%)\n",
      "\n",
      "Iteration 300, training loss = 2.2239\n",
      "Loss: 2.1809116757434346\n",
      "Got 920 / 4600 correct (20.0%)\n",
      "\n",
      "Iteration 350, training loss = 2.2437\n",
      "Loss: 2.1848693878754326\n",
      "Got 900 / 4600 correct (19.565217391304348%)\n",
      "\n",
      "Loss: 2.1837060503337695\n",
      "Got 900 / 4600 correct (19.565217391304348%)\n",
      "******************************\n",
      "End of epoch 18 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.2315289014516586, accuracy: 0.20303432086421633\n",
      "Val loss: 2.1837060503337695, accuracy: 0.1956521739130435\n",
      "******************************\n",
      "----------\n",
      "Epoch 19\n",
      "----------\n",
      "Iteration 0, training loss = 2.2388\n",
      "Loss: 2.184215193209441\n",
      "Got 897 / 4600 correct (19.5%)\n",
      "\n",
      "Iteration 50, training loss = 2.2287\n",
      "Loss: 2.1829537827035654\n",
      "Got 886 / 4600 correct (19.26086956521739%)\n",
      "\n",
      "Iteration 100, training loss = 2.2390\n",
      "Loss: 2.180532854536305\n",
      "Got 884 / 4600 correct (19.217391304347824%)\n",
      "\n",
      "Iteration 150, training loss = 2.2137\n",
      "Loss: 2.180195849874745\n",
      "Got 902 / 4600 correct (19.608695652173914%)\n",
      "\n",
      "Iteration 200, training loss = 2.1980\n",
      "Loss: 2.1814317288606064\n",
      "Got 905 / 4600 correct (19.67391304347826%)\n",
      "\n",
      "Iteration 250, training loss = 2.2105\n",
      "Loss: 2.179802847945172\n",
      "Got 912 / 4600 correct (19.82608695652174%)\n",
      "\n",
      "Iteration 300, training loss = 2.2675\n",
      "Loss: 2.181642314662104\n",
      "Got 911 / 4600 correct (19.804347826086957%)\n",
      "\n",
      "Iteration 350, training loss = 2.2633\n",
      "Loss: 2.18008173548657\n",
      "Got 922 / 4600 correct (20.043478260869566%)\n",
      "\n",
      "Loss: 2.1800291538238525\n",
      "Got 914 / 4600 correct (19.869565217391305%)\n",
      "******************************\n",
      "End of epoch 19 summary\n",
      "Total samples: 184028\n",
      "Training loss: 2.231306629972545, accuracy: 0.20269741561066795\n",
      "Val loss: 2.1800291538238525, accuracy: 0.19869565217391305\n",
      "******************************\n",
      "Training complete in 159m 5s\n",
      "Best accuracy: 0.21239130434782608\n",
      "Test accuracy\n",
      "Loss: 2.2173875518467114\n",
      "Got 986 / 4600 correct (21.434782608695652%)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
